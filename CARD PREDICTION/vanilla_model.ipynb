{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "import keras\n",
    "from keras.models import Model,Sequential\n",
    "from tensorflow.keras import layers, models,optimizers\n",
    "from keras.layers import Input,Dense,Conv2D,AveragePooling2D,Flatten,MaxPooling2D,BatchNormalization,Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-af7e6f17f6af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\\{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\\{}\\{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtraining_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "\n",
    "training_dict = dict()\n",
    "\n",
    "training_data = []\n",
    "training_cv = []\n",
    "\n",
    "for folder in os.listdir(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\"):\n",
    "    count=0\n",
    "    for image in os.listdir(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\\{}\".format(folder)):\n",
    "        training_data.append(cv2.imread(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\\{}\\{}\".format(folder,image)))\n",
    "        training_cv.append(folder)\n",
    "        count+=1\n",
    "    training_dict[folder]=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "\n",
    "training_dict = dict()\n",
    "\n",
    "training_data = []\n",
    "training_cv = []\n",
    "\n",
    "for folder in os.listdir(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\"):\n",
    "    count=0\n",
    "    limit = 0\n",
    "    for image in os.listdir(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\\{}\".format(folder)):\n",
    "        \n",
    "        if limit <100:\n",
    "            training_data.append(cv2.imread(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\\{}\\{}\".format(folder,image)))\n",
    "            training_cv.append(folder)\n",
    "            count+=1\n",
    "            limit+=1\n",
    "    training_dict[folder]=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5300\n",
      "5300\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "print(len(training_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation Data\n",
    "validation_data = []\n",
    "validation_cv = []\n",
    "\n",
    "for folder in os.listdir(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\valid\"):\n",
    "    for image in os.listdir(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\valid\\{}\".format(folder)):\n",
    "        validation_data.append(cv2.imread(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\valid\\{}\\{}\".format(folder,image)))\n",
    "        validation_cv.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "265\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_data))\n",
    "print(len(validation_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Data\n",
    "test_data = []\n",
    "test_cv = []\n",
    "\n",
    "for folder in os.listdir(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\test\"):\n",
    "    for image in os.listdir(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\test\\{}\".format(folder)):\n",
    "        test_data.append(cv2.imread(r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\test\\{}\\{}\".format(folder,image)))\n",
    "        test_cv.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "262\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))\n",
    "print(len(test_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.asarray(training_data)\n",
    "validation_data = np.asarray(validation_data)\n",
    "test_data = np.asarray(test_data)\n",
    "\n",
    "training_cv = np.asarray(training_cv)\n",
    "validation_cv = np.asarray(validation_cv)\n",
    "test_cv = np.asarray(test_cv)\n",
    "\n",
    "le = LabelEncoder()\n",
    "training_cv = le.fit_transform(training_cv)\n",
    "validation_cv = le.transform(validation_cv)\n",
    "test_cv = le.transform(test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-4071a1fba099>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-4071a1fba099>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    --\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some images from training data\n",
    "\n",
    "list_index = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        index_ = random.randint(0, len(training_data) - 1)\n",
    "        axes[i, j].imshow(training_data[index_])\n",
    "        axes[i, j].axis('off')  \n",
    "        axes[i, j].set_title('Image ' + str(index_)) \n",
    "        list_index.append(index_)\n",
    "        \n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_index:\n",
    "    print('Shape of the training image is :',training_data[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some images from validation data\n",
    "\n",
    "list_index = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        index_ = random.randint(0, len(validation_data) - 1)\n",
    "        axes[i, j].imshow(validation_data[index_])\n",
    "        axes[i, j].axis('off')  \n",
    "        axes[i, j].set_title('Image ' + str(index_)) \n",
    "        list_index.append(index_)\n",
    "        \n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_index:\n",
    "    print('Shape of the validation image is :',validation_data[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some images from test data\n",
    "\n",
    "list_index = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        index_ = random.randint(0, len(test_data) - 1)\n",
    "        axes[i, j].imshow(test_data[index_])\n",
    "        axes[i, j].axis('off')  \n",
    "        axes[i, j].set_title('Image ' + str(index_)) \n",
    "        list_index.append(index_)\n",
    "        \n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_index:\n",
    "    print('Shape of the test_data image is :',test_data[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_distribution = pd.DataFrame(list(training_dict.items()), columns=['class label', 'count'])\n",
    "data_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of training data is :',len(training_data))\n",
    "print('length of validation data is :',len(validation_data))\n",
    "print('length of test data is :',len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\\ace of hearts\\003.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# 1. Visual Inspection\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# 2. Sharpness Analysis\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "laplacian_var = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "print(\"Laplacian Variance (Sharpness):\", laplacian_var)\n",
    "\n",
    "# 3. Noise Analysis (Example: Salt and Pepper Noise)\n",
    "noisy_image = image.copy()\n",
    "noise = np.random.randint(0, 2, size=image.shape[:2]) * 255\n",
    "noisy_image[np.where(noise == 255)] = [255, 255, 255]  \n",
    "noisy_image[np.where(noise == 0)] = [0, 0, 0]\n",
    "\n",
    "plt.imshow(noisy_image)\n",
    "plt.show()\n",
    "\n",
    "# 4. Resolution and Compression\n",
    "resolution = image.shape[:2]\n",
    "print(\"Image Resolution:\", resolution)\n",
    "\n",
    "# 5. Color Balance (Example: Histogram Equalization)\n",
    "lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "lab_planes = cv2.split(lab_image)\n",
    "lab_planes[0] = cv2.equalizeHist(lab_planes[0])\n",
    "equalized_lab = cv2.merge(lab_planes)\n",
    "equalized_image = cv2.cvtColor(equalized_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "plt.imshow(equalized_image)\n",
    "plt.show()\n",
    "\n",
    "# 6. Uniformity (Example: Histogram Analysis)\n",
    "histogram = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "uniformity_score = np.sum(np.abs(histogram - np.mean(histogram)))\n",
    "print(\"Uniformity Score:\", uniformity_score)\n",
    "\n",
    "# 7. Blur Detection (Example: Laplacian Variance)\n",
    "laplacian_var = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "print(\"Laplacian Variance (Blur):\", laplacian_var)\n",
    "\n",
    "# 8. Artifact Detection (Example: Edge Detection)\n",
    "edges = cv2.Canny(gray_image, 100, 200)\n",
    "plt.imshow(edges)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Assuming you have a list of images and their corresponding labels\n",
    "# Replace 'images' and 'labels' with your actual data\n",
    "images = [...]  # List of image arrays\n",
    "labels = [...]  # List of corresponding labels\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Data augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,      # Random rotation (±20 degrees)\n",
    "    width_shift_range=0.1,  # Random horizontal shift\n",
    "    height_shift_range=0.1, # Random vertical shift\n",
    "    shear_range=0.2,        # Shear transformation\n",
    "    zoom_range=0.2,         # Random zoom\n",
    "    horizontal_flip=True,   # Random horizontal flip\n",
    "    vertical_flip=True,     # Random vertical flip\n",
    "    fill_mode='nearest'     # Fill mode for points outside the input boundaries\n",
    ")\n",
    "\n",
    "# Create a data generator with balanced classes\n",
    "balanced_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    # Class weights to balance out imbalanced classes\n",
    "    class_weight={0: 1, 1: 1, 2: 1, ...}  # Replace 0, 1, 2, ... with your class labels\n",
    ")\n",
    "\n",
    "# Example usage of data augmentation\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "# Generate augmented images and labels\n",
    "for X_batch, y_batch in datagen.flow(images, labels, batch_size=32):\n",
    "    # Append augmented images and labels\n",
    "    augmented_images.append(X_batch)\n",
    "    augmented_labels.append(y_batch)\n",
    "    # Break the loop if enough augmented data is generated\n",
    "    if len(augmented_images) >= desired_augmented_data_size:\n",
    "        break\n",
    "\n",
    "# Concatenate the augmented data\n",
    "augmented_images = np.concatenate(augmented_images)\n",
    "augmented_labels = np.concatenate(augmented_labels)\n",
    "\n",
    "# Example usage of balanced data generator\n",
    "balanced_datagen.fit(images)  # Fit the data generator to your original data\n",
    "balanced_data_generator = balanced_datagen.flow(images, labels, batch_size=32)\n",
    "\n",
    "# Use the balanced data generator for training\n",
    "model.fit(balanced_data_generator, epochs=10, steps_per_epoch=len(images) // 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rotation_range = 20,zoom_range=0.2, horizontal_flip=False,\n",
    "                                  shear_range=0.2,rescale=1./255)\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path1 = r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\train\"\n",
    "\n",
    "valid_path1 = r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\valid\"\n",
    "\n",
    "test_path1 = r\"C:\\Users\\LAKSHMI NARASIMHARAO\\innomatics\\Deep Learning\\Projects\\Cards prediction\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7624 images belonging to 53 classes.\n",
      "Found 265 images belonging to 53 classes.\n",
      "Found 262 images belonging to 53 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set1 = train_gen.flow_from_directory(train_path1,\n",
    "                                              target_size=(224,224),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode = 'categorical')\n",
    "\n",
    "valid_set1 = test_gen.flow_from_directory(valid_path1,\n",
    "                                            target_size=(224,224),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='categorical')\n",
    "\n",
    "test_set1 = test_gen.flow_from_directory(test_path1,\n",
    "                                            target_size=(224,224),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "106/106 [==============================] - 1130s 11s/step - loss: 25.9207 - accuracy: 0.0453 - val_loss: 3.7345 - val_accuracy: 0.1094\n",
      "Epoch 2/15\n",
      "106/106 [==============================] - 763s 7s/step - loss: 3.6727 - accuracy: 0.0974 - val_loss: 3.3368 - val_accuracy: 0.1434\n",
      "Epoch 3/15\n",
      "106/106 [==============================] - 608s 6s/step - loss: 3.4454 - accuracy: 0.1442 - val_loss: 3.2416 - val_accuracy: 0.1887\n",
      "Epoch 4/15\n",
      "106/106 [==============================] - 826s 8s/step - loss: 3.1709 - accuracy: 0.1955 - val_loss: 3.0790 - val_accuracy: 0.2038\n",
      "Epoch 5/15\n",
      "106/106 [==============================] - 1057s 10s/step - loss: 2.9894 - accuracy: 0.2360 - val_loss: 2.9816 - val_accuracy: 0.2377\n",
      "Epoch 6/15\n",
      "106/106 [==============================] - 1068s 10s/step - loss: 2.7761 - accuracy: 0.2798 - val_loss: 2.6826 - val_accuracy: 0.3094\n",
      "Epoch 7/15\n",
      "106/106 [==============================] - 1067s 10s/step - loss: 2.5762 - accuracy: 0.3151 - val_loss: 2.5822 - val_accuracy: 0.3208\n",
      "Epoch 8/15\n",
      "106/106 [==============================] - 762s 7s/step - loss: 2.3950 - accuracy: 0.3540 - val_loss: 2.6349 - val_accuracy: 0.3019\n",
      "Epoch 9/15\n",
      "106/106 [==============================] - 617s 6s/step - loss: 2.2364 - accuracy: 0.3864 - val_loss: 2.6072 - val_accuracy: 0.3132\n",
      "Epoch 10/15\n",
      "106/106 [==============================] - 613s 6s/step - loss: 2.1322 - accuracy: 0.4119 - val_loss: 2.5365 - val_accuracy: 0.3170\n",
      "Epoch 11/15\n",
      "106/106 [==============================] - 611s 6s/step - loss: 2.0699 - accuracy: 0.4321 - val_loss: 2.5317 - val_accuracy: 0.3396\n",
      "Epoch 12/15\n",
      "106/106 [==============================] - 610s 6s/step - loss: 1.9373 - accuracy: 0.4625 - val_loss: 2.6428 - val_accuracy: 0.3358\n",
      "Epoch 13/15\n",
      "106/106 [==============================] - 615s 6s/step - loss: 1.8682 - accuracy: 0.4689 - val_loss: 2.6720 - val_accuracy: 0.3396\n",
      "Epoch 14/15\n",
      "106/106 [==============================] - 861s 8s/step - loss: 1.8419 - accuracy: 0.4889 - val_loss: 2.6112 - val_accuracy: 0.3585\n",
      "Epoch 15/15\n",
      "106/106 [==============================] - 1069s 10s/step - loss: 1.7514 - accuracy: 0.5072 - val_loss: 2.5627 - val_accuracy: 0.3396\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=training_data.shape[1:]),  # Update input shape\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),  # Increased number of units\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),  # Increased number of units\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(53, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(training_data, training_cv, batch_size=50, epochs=15,  # Increased number of epochs\n",
    "                    validation_data=(validation_data, validation_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 14s 2s/step - loss: 2.6591 - accuracy: 0.3702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.659060478210449, 0.37022900581359863]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data,test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Vanilla.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
